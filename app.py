from dotenv import load_dotenv

load_dotenv()

# app.py
# -------------------------------------------------------
# ä½¿ã„æ–¹ï¼š
# 1) ã€Œå°‚é–€å®¶ãƒ¢ãƒ¼ãƒ‰ã€ã‚’ A / B ã‹ã‚‰é¸ã¶
# 2) ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›
# 3) é€ä¿¡ã§ã€é¸ã‚“ã å°‚é–€å®¶ã®è¦³ç‚¹ã§å›ç­”ãŒè¡¨ç¤ºã•ã‚Œã‚‹
# -------------------------------------------------------

# --- æŒ‡ç¤ºã©ãŠã‚Šã® dotenv å‘¼ã³å‡ºã—ã€‚ãŸã ã—æœªå°å…¥ã§ã‚‚è½ã¡ãªã„ã‚ˆã†ã« no-op ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ ---
try:
    from dotenv import load_dotenv  # å¤–éƒ¨ä¾å­˜
except Exception:
    def load_dotenv(*args, **kwargs):
        return False
load_dotenv()

import os               # æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã¿
import importlib        # æ¨™æº–
import traceback        # æ¨™æº–
import streamlit as st  # Cloud æ—¢å®šã§åˆ©ç”¨å¯èƒ½

# Streamlit ã®æœ€åˆã®å‘¼ã³å‡ºã—ã¯ page_configï¼ˆä»–ã® st.* ã‚ˆã‚Šå‰ï¼‰
try:
    st.set_page_config(page_title="Expert Mode LLM App", page_icon="ğŸ§ ", layout="centered")
except Exception:
    pass  # å¤ã„Streamlitã§ã‚‚è½ã¨ã•ãªã„

# -------------------------
# ç”»é¢ãƒ˜ãƒƒãƒ€ï¼ˆã“ã“ã¾ã§ã§ UI ã¯å¿…ãšè¡¨ç¤ºã•ã‚Œã‚‹ï¼‰
# -------------------------
st.title("ğŸ§  Expert Mode LLM App")
st.caption("A/B ã‹ã‚‰å°‚é–€å®¶ãƒ¢ãƒ¼ãƒ‰ã‚’é¸ã³ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’é€ä¿¡ã™ã‚‹ã¨ã€é¸ã‚“ã å°‚é–€å®¶ã®è¦³ç‚¹ã§å›ç­”ã—ã¾ã™ã€‚")

with st.expander("â„¹ï¸ ã“ã®ã‚¢ãƒ—ãƒªã®æ¦‚è¦ã¨æ“ä½œæ–¹æ³•", expanded=False):
    st.markdown(
        "- **æ¦‚è¦**: å˜ä¸€å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ï¼‹å°‚é–€å®¶ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ã§ã€LangChain ã‚’é€šã—ã¦ LLM ã«è³ªå•ã—ã€çµæœã‚’è¡¨ç¤ºã€‚\n"
        "- **æ“ä½œ**: ãƒ©ã‚¸ã‚ªã§ãƒ¢ãƒ¼ãƒ‰é¸æŠ â†’ ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ› â†’ **é€ä¿¡** ãƒœã‚¿ãƒ³ã€‚\n"
        "- **æ³¨æ„**: ç”Ÿæˆçµæœã¯ä¸€èˆ¬çš„æƒ…å ±ã§ã‚ã‚Šã€æœ€çµ‚åˆ¤æ–­ã¯è‡ªå·±è²¬ä»»ã§ã€‚"
    )

# -------------------------
# å°‚é–€å®¶ãƒ¢ãƒ¼ãƒ‰å®šç¾©ï¼ˆè¦ä»¶ï¼šé¸æŠã§ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ‡æ›¿ï¼‰
# -------------------------
EXPERTS = {
    "A": {
        "label": "Aï½œç”ŸæˆAIå®Ÿè£…ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ",
        "system": (
            "ã‚ãªãŸã¯ç”ŸæˆAI/LLMå®Ÿè£…ã«å¼·ã„ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã‚ã‚‹ã€‚"
            "è¦ä»¶å®šç¾©â†’è¨­è¨ˆâ†’å®Ÿè£…â†’è©•ä¾¡ã®é †ã§ã€çŸ­ãå…·ä½“çš„ã«åŠ©è¨€ã™ã‚‹ã€‚"
            "ç®‡æ¡æ›¸ãã‚’å¥½ã¿ã€å‰æãƒ»åˆ¶ç´„ãƒ»ãƒªã‚¹ã‚¯ã‚‚ç°¡æ½”ã«è§¦ã‚Œã‚‹ã€‚"
            "å†—é•·ãªæ¯”å–©ã¯é¿ã‘ã€æ—¥æœ¬èªã§æ˜å¿«ã«è¿°ã¹ã‚‹ã€‚"
        ),
    },
    "B": {
        "label": "Bï½œç©ºæ¸¯ã‚¢ã‚¯ã‚»ã‚¹ãƒ»ãƒ¢ãƒ“ãƒªãƒ†ã‚£ã‚¢ãƒŠãƒªã‚¹ãƒˆ",
        "system": (
            "ã‚ãªãŸã¯ç©ºæ¸¯ã‚¢ã‚¯ã‚»ã‚¹/ãƒ¢ãƒ“ãƒªãƒ†ã‚£ã®ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã‚ã‚‹ã€‚"
            "äº¤é€šãƒ¢ãƒ¼ãƒ‰æ¯”è¼ƒã€éœ€è¦è¦‹ç«‹ã¦ã€é‹ç”¨ãƒ»åç›Šã®è¦³ç‚¹ã‹ã‚‰åŠ©è¨€ã™ã‚‹ã€‚"
            "æ¨å®šå€¤ã¯æ ¹æ‹ ã¨ä¸ç¢ºå®Ÿæ€§ã‚’æ˜ç¤ºã—ã€æ—¥æœ¬èªã§ç°¡æ½”ã«è¿°ã¹ã‚‹ã€‚"
        ),
    },
}

# -------------------------
# APIã‚­ãƒ¼å–å¾—ï¼ˆCloudã§ã‚‚è½ã¡ãªã„ã€‚Secretså„ªå…ˆâ†’ç’°å¢ƒå¤‰æ•°ï¼‰
# -------------------------
def resolve_openai_api_key():
    try:
        if hasattr(st, "secrets"):
            key = st.secrets.get("OPENAI_API_KEY", None)
            if not key and "openai" in st.secrets:
                blk = st.secrets.get("openai", {})
                if isinstance(blk, dict):
                    key = blk.get("api_key")
            if key:
                return key, "secrets"
    except Exception:
        pass
    key = os.getenv("OPENAI_API_KEY")
    if key:
        return key, "env"
    return None, "missing"

# -------------------------
# LangChain é…å»¶ importï¼ˆæ–°æ—§APIã‚’æ¢ç´¢ï¼‰ã€‚å¤±æ•—ã—ã¦ã‚‚ UI ã¯ç¶­æŒã€‚
# -------------------------
def lazy_import_langchain():
    errors = []

    ChatOpenAI = None
    for path, name in [
        ("langchain_openai", "ChatOpenAI"),                 # æ–°API
        ("langchain.chat_models", "ChatOpenAI"),            # æ—§APIï¼ˆåˆ†å‰²å‰ï¼‰
        ("langchain_community.chat_models", "ChatOpenAI"),  # æ—§APIï¼ˆåˆ†å‰²æœŸï¼‰
    ]:
        try:
            mod = importlib.import_module(path)
            ChatOpenAI = getattr(mod, name)
            break
        except Exception as e:
            errors.append(f"{path}.{name}: {e}")

    SystemMessage = HumanMessage = None
    for path, sys_name, hum_name in [
        ("langchain_core.messages", "SystemMessage", "HumanMessage"),
        ("langchain.schema", "SystemMessage", "HumanMessage"),
    ]:
        try:
            mod = importlib.import_module(path)
            SystemMessage = getattr(mod, sys_name)
            HumanMessage = getattr(mod, hum_name)
            break
        except Exception as e:
            errors.append(f"{path}: {e}")

    if ChatOpenAI and SystemMessage and HumanMessage:
        return ChatOpenAI, SystemMessage, HumanMessage, None
    return None, None, None, "\n".join(errors) if errors else "unknown import error"

# -------------------------
# è¦ä»¶ã®é–¢æ•°ï¼šå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆï¼‹é¸æŠå€¤ â†’ æ–‡å­—åˆ—ã§å›ç­”
# -------------------------
def run_llm(user_text: str, expert_key: str) -> str:
    # 1) APIã‚­ãƒ¼ç¢ºèªï¼ˆæœªè¨­å®šã§ã‚‚ UI ã¯ä¿ã¤ï¼‰
    api_key, key_source = resolve_openai_api_key()
    if not api_key:
        return "OpenAI APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚Cloudã§ã¯ App â†’ Settings â†’ Secrets ã« `OPENAI_API_KEY` ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
    os.environ["OPENAI_API_KEY"] = api_key  # LangChain/SDKãŒå‚ç…§

    # 2) LangChain ã‚’é…å»¶èª­ã¿è¾¼ã¿
    ChatOpenAI, SystemMessage, HumanMessage, import_err = lazy_import_langchain()
    if import_err:
        return (
            "LangChain ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚requirements.txt ã‚’æ¬¡ã®ä¾‹ã§ç”¨æ„ã—ã¦ãã ã•ã„ï¼š\n"
            "streamlit>=1.36\n"
            "langchain>=0.3.0\n"
            "langchain-openai>=0.1.21\n"
            "openai>=1.51.0\n"
            "python-dotenv>=1.0.1\n\n"
            f"è©³ç´°: {import_err}"
        )

    # 3) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã‚·ã‚¹ãƒ†ãƒ ï¼‹ãƒ¦ãƒ¼ã‚¶ï¼‰
    system_msg = EXPERTS.get(expert_key, EXPERTS["A"])["system"]
    messages = [SystemMessage(content=system_msg), HumanMessage(content=user_text)]

    # 4) ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆå¼•æ•°å·®ã‚’å¸åï¼‰
    llm = None
    last_err = None
    for kwargs in (
        {"model": "gpt-4o-mini", "temperature": 0.3},
        {"model_name": "gpt-4o-mini", "temperature": 0.3},
        {"model": "gpt-4o-mini", "temperature": 0.3, "openai_api_key": api_key},
        {"model_name": "gpt-4o-mini", "temperature": 0.3, "openai_api_key": api_key},
    ):
        try:
            llm = ChatOpenAI(**kwargs)
            break
        except Exception as e:
            last_err = e
    if llm is None:
        return f"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆgpt-4o-miniï¼‰ã€‚è©³ç´°: {last_err}"

    # 5) å‘¼ã³å‡ºã—ï¼ˆinvoke / __call__ / predict_messages / generate ã®é †ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    try:
        try:
            res = llm.invoke(messages)  # æ–°API
        except Exception:
            try:
                res = llm(messages)      # æ—§API
            except Exception:
                if hasattr(llm, "predict_messages"):
                    res = llm.predict_messages(messages)
                elif hasattr(llm, "generate"):
                    res = llm.generate([messages])  # LLMResult
                else:
                    raise RuntimeError("äº’æ›å‘¼ã³å‡ºã—ãƒ¡ã‚½ãƒƒãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    except Exception as e:
        tb = traceback.format_exc(limit=2)
        return f"LLMå‘¼ã³å‡ºã—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚è©³ç´°: {e}\n{tb}"

    # 6) å‡ºåŠ›æŠ½å‡º
    try:
        if hasattr(res, "content"):
            answer = res.content
        elif isinstance(res, dict) and "content" in res:
            answer = res["content"]
        elif hasattr(res, "generations"):  # LLMResult
            gens = getattr(res, "generations", None)
            if gens and gens[0]:
                g0 = gens[0][0]
                if hasattr(g0, "message") and hasattr(g0.message, "content"):
                    answer = g0.message.content
                elif hasattr(g0, "text"):
                    answer = g0.text
                else:
                    answer = str(res)
            else:
                answer = str(res)
        else:
            answer = str(res)
    except Exception:
        answer = str(res)

    suffix = f"\n\n---\nï¼ˆä½¿ç”¨ãƒ¢ãƒ¼ãƒ‰: {EXPERTS[expert_key]['label']}ï½œãƒ¢ãƒ‡ãƒ«: gpt-4o-miniï½œã‚­ãƒ¼å–å¾—å…ƒ: {key_source}ï¼‰"
    return answer + suffix

# -------------------------
# ãƒ•ã‚©ãƒ¼ãƒ ï¼ˆã“ã“ã¾ã§ã§ UI ã¯ç¢ºå®Ÿã«è¡¨ç¤ºã•ã‚Œã‚‹ï¼‰
# -------------------------
with st.form("main_form", clear_on_submit=False):
    labels = [v["label"] for v in EXPERTS.values()]
    key_by_label = {v["label"]: k for k, v in EXPERTS.items()}
    # horizontal=True ã¯å¤ã„Streamlitã§è½ã¡ã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚æœªä½¿ç”¨
    selected_label = st.radio("å°‚é–€å®¶ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ", labels, index=0)
    selected_key = key_by_label[selected_label]

    user_text = st.text_area(
        "å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ",
        placeholder="ã“ã“ã«è³ªå•ã‚„èª²é¡Œã€è¦ä»¶ãªã©ã‚’è¨˜å…¥ã—ã¦ãã ã•ã„ã€‚",
        height=180,
    )
    submitted = st.form_submit_button("é€ä¿¡")

# -------------------------
# å®Ÿè¡Œã¨è¡¨ç¤ºï¼ˆä¾‹å¤–ã¯ç”»é¢å†…ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦è¿”ã™ï¼ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ãªã„ï¼‰
# -------------------------
if submitted:
    if not user_text.strip():
        st.warning("ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("LLMãŒå›ç­”ã‚’ç”Ÿæˆä¸­â€¦"):
            answer = run_llm(user_text=user_text, expert_key=selected_key)
            st.markdown("### âœ… å›ç­”")
            st.write(answer)

# -------------------------
# èµ·å‹•æ™‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºï¼ˆç”»é¢ä¸‹éƒ¨ã«æƒ…å ±è¡¨ç¤ºã®ã¿ï¼‰
# -------------------------
with st.expander("ğŸ”§ è¨­å®šã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼ˆè¡¨ç¤ºã®ã¿ï¼å®‰å…¨ï¼‰", expanded=False):
    key, src = resolve_openai_api_key()
    st.write(f"- OPENAI_API_KEY æ¤œå‡º: {'ã¯ã„' if key else 'ã„ã„ãˆ'}ï¼ˆã‚½ãƒ¼ã‚¹: {src}ï¼‰")
    st.write("- LangChain / OpenAI ã¯é€ä¿¡æ™‚ã«è‡ªå‹•èª­ã¿è¾¼ã¿ã—ã¾ã™ã€‚æœªå°å…¥ã§ã‚‚UIã¯è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
